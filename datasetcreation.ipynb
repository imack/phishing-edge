{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import psycopg2\n",
    "import boto3\n",
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from bs4 import BeautifulSoup\n",
    "from io import BytesIO\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Database and AWS configuration\n",
    "AWS_S3_BUCKET = 'stingray-phishing-dataset'\n",
    "HDF5_FILE_PATH = 'phishing.h5.nosync'"
   ],
   "id": "e6feaa922ed00a86",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Connect to the database\n",
    "def get_db_connection():\n",
    "    db_string = os.getenv('RDS_CONNECTION')\n",
    "    return psycopg2.connect(db_string)"
   ],
   "id": "9022bec0842dae06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Fetch entries from the database\n",
    "def fetch_entries():\n",
    "    with get_db_connection() as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\"\"\"\n",
    "                SELECT id, full_url, s3_html_key, s3_screenshot_key, source\n",
    "                FROM urls\n",
    "                WHERE s3_html_key IS NOT NULL AND phash_distance > 10 AND source IN (1, 3, 4, 5)\n",
    "            \"\"\")\n",
    "            return cur.fetchall()"
   ],
   "id": "2a810fe02172449b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create or open the HDF5 file\n",
    "h5_file = h5py.File(HDF5_FILE_PATH, 'a')\n",
    "\n",
    "if 'urls' not in h5_file:\n",
    "    h5_file.create_dataset('urls', (0,), maxshape=(None,), dtype=h5py.string_dtype())\n",
    "    h5_file.create_dataset('screenshots', (0, 340, 680, 3), maxshape=(None, 340, 680, 3), dtype='uint8')\n",
    "    h5_file.create_dataset('html_content', (0,), maxshape=(None,), dtype=h5py.string_dtype())\n",
    "    h5_file.create_dataset('source', (0,), maxshape=(None,), dtype='i') \n",
    "    # { internal: 0, open_phish: 1, api: 2, alexa: 3, product_hunt: 4, expansion: 5 }\n"
   ],
   "id": "3773d986896376e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import redis\n",
    "from collections import Counter\n",
    "\n",
    "r = redis.Redis(host='localhost', port=6379, db=0)\n",
    "# Define expiry time (30 days in seconds)\n",
    "expiry_time = 30 * 24 * 60 * 60  # 30 days\n",
    "namespace = \"phish_dataset\"\n"
   ],
   "id": "4cd7cddee9e45276",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time \n",
    "from urllib.parse import urlparse\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "urls_dataset = h5_file['urls']\n",
    "screenshots_dataset = h5_file['screenshots']\n",
    "html_content_dataset = h5_file['html_content']\n",
    "source_dataset = h5_file['source']\n",
    "\n",
    "start_time = time.time()\n",
    "count = 0\n",
    "# Process entries\n",
    "for entry in fetch_entries():\n",
    "    url_id, full_url, s3_html_key, s3_screenshot_key, source = entry\n",
    "    count += 1\n",
    "        \n",
    "    if count % 1000 == 0:\n",
    "        h5_file.flush()\n",
    "        execution_time = time.time() - start_time\n",
    "        print(\"\")\n",
    "        print(f\"Count {count}; {execution_time} seconds\")\n",
    "        time.sleep(5)\n",
    "        start_time = time.time()\n",
    "    \n",
    "    \n",
    "    parsed_url = urlparse(full_url)\n",
    "    \n",
    "    # Construct the URL without query parameters\n",
    "    target_url = f\"{parsed_url.scheme}://{parsed_url.netloc}{parsed_url.path}\"\n",
    "    redis_key = f\"{namespace}:{target_url}\"\n",
    "    \n",
    "    # Get the domain\n",
    "    domain = parsed_url.netloc\n",
    "    \n",
    "    # Check if URL is already in HDF5 dataset\n",
    "    if r.exists(redis_key):\n",
    "        print(\"-\", end =\"\")\n",
    "        continue\n",
    "        \n",
    "    blocked_domains = [\"pages.dev\", \"github.io\", \"weebly.com\", \"vercel.app\", \"weeblysite.com\", \"gitbook.io\" ]\n",
    "    if any(blocked_domain in domain for blocked_domain in blocked_domains):\n",
    "        r.setex(redis_key, expiry_time, target_url)\n",
    "        print(\"x\", end =\"\")\n",
    "        continue\n",
    "    \n",
    "    # Download screenshot from S3\n",
    "    try:\n",
    "        screenshot_obj = s3_client.get_object(Bucket=AWS_S3_BUCKET, Key=s3_screenshot_key)\n",
    "        screenshot_image = Image.open(BytesIO(screenshot_obj['Body'].read()))\n",
    "        screenshot_image = screenshot_image.resize((680, 340))  # Resize image to 680x340\n",
    "        screenshot_array = np.array(screenshot_image)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching screenshot for URL {target_url}: {e}\")\n",
    "        continue\n",
    "        \n",
    "    # Download and parse HTML from S3\n",
    "    try:\n",
    "        html_obj = s3_client.get_object(Bucket=AWS_S3_BUCKET, Key=s3_html_key)\n",
    "        html_content = html_obj['Body'].read().decode('utf-8')\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        main_html_content = soup.prettify()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching HTML for URL {target_url}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    new_index = urls_dataset.shape[0]\n",
    "    urls_dataset.resize((new_index + 1,))\n",
    "    urls_dataset[new_index] = target_url\n",
    "    \n",
    "    source_dataset.resize((new_index + 1,))\n",
    "    source_dataset[new_index] = source\n",
    "    \n",
    "    screenshots_dataset.resize((new_index + 1, 340, 680, 3))\n",
    "    screenshots_dataset[new_index] = screenshot_array[:, :, :3]\n",
    "\n",
    "    html_content_dataset.resize((new_index + 1,))\n",
    "    html_content_dataset[new_index] = main_html_content.replace('\\x00', '')\n",
    "\n",
    "    \n",
    "    r.setex(redis_key, expiry_time, target_url)\n",
    "    print(\".\", end =\"\")\n",
    "\n",
    "# Close HDF5 file\n",
    "h5_file.close()\n"
   ],
   "id": "58f6ec9229270d33",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....xx............................x......x.......................x.."
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T14:28:21.664808Z",
     "start_time": "2024-10-24T14:28:21.660135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "h5_file = h5py.File(HDF5_FILE_PATH, 'a')\n",
    "urls_dataset = h5_file['urls']\n",
    "screenshots_dataset = h5_file['screenshots']\n",
    "html_content_dataset = h5_file['html_content']\n",
    "source_dataset = h5_file['source']\n",
    "\n",
    "# Use Counter to count the frequency of each unique value\n",
    "print(source_dataset.shape)\n",
    "print(urls_dataset.shape)\n",
    "print(screenshots_dataset.shape)\n",
    "print(html_content_dataset.shape)"
   ],
   "id": "9e0a334f60bd99e7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72083,)\n",
      "(72083,)\n",
      "(72083, 340, 680, 3)\n",
      "(72083,)\n"
     ]
    }
   ],
   "execution_count": 49
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
