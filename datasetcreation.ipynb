{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-23T00:45:18.674680Z",
     "start_time": "2024-10-23T00:45:18.366901Z"
    }
   },
   "source": [
    "import psycopg2\n",
    "import boto3\n",
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from bs4 import BeautifulSoup\n",
    "from io import BytesIO\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T00:45:18.689925Z",
     "start_time": "2024-10-23T00:45:18.687660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Database and AWS configuration\n",
    "AWS_S3_BUCKET = 'stingray-phishing-dataset'\n",
    "HDF5_FILE_PATH = 'phishing.h5.nosync'"
   ],
   "id": "e6feaa922ed00a86",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T00:45:18.736813Z",
     "start_time": "2024-10-23T00:45:18.734284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Connect to the database\n",
    "def get_db_connection():\n",
    "    db_string = os.getenv('RDS_CONNECTION')\n",
    "    return psycopg2.connect(db_string)"
   ],
   "id": "9022bec0842dae06",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T00:45:18.745235Z",
     "start_time": "2024-10-23T00:45:18.742864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fetch entries from the database\n",
    "def fetch_entries():\n",
    "    with get_db_connection() as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\"\"\"\n",
    "                SELECT id, full_url, s3_html_key, s3_screenshot_key, source, status\n",
    "                FROM urls\n",
    "                WHERE s3_html_key IS NOT NULL AND phash_distance > 10\n",
    "            \"\"\")\n",
    "            return cur.fetchall()"
   ],
   "id": "2a810fe02172449b",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T00:45:18.758459Z",
     "start_time": "2024-10-23T00:45:18.751094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create or open the HDF5 file\n",
    "h5_file = h5py.File(HDF5_FILE_PATH, 'a')\n",
    "\n",
    "if 'urls' not in h5_file:\n",
    "    h5_file.create_dataset('urls', (0,), maxshape=(None,), dtype=h5py.string_dtype())\n",
    "if 'screenshots' not in h5_file:\n",
    "    h5_file.create_dataset('screenshots', (0, 340, 680, 3), maxshape=(None, 340, 680, 3), dtype='uint8')\n",
    "if 'html_content' not in h5_file:\n",
    "    h5_file.create_dataset('html_content', (0,), maxshape=(None,), dtype=h5py.string_dtype())\n",
    "if 'status' not in h5_file:\n",
    "    h5_file.create_dataset('status', (0,), maxshape=(None,), dtype='i')\n",
    "if 'source' not in h5_file:\n",
    "    h5_file.create_dataset('source', (0,), maxshape=(None,), dtype='i')\n",
    "        \n"
   ],
   "id": "3773d986896376e1",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-10-23T00:57:00.399490Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time \n",
    "from urllib.parse import urlparse\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "urls_dataset = h5_file['urls']\n",
    "screenshots_dataset = h5_file['screenshots']\n",
    "html_content_dataset = h5_file['html_content']\n",
    "status_dataset = h5_file['status']\n",
    "source_dataset = h5_file['source']\n",
    "\n",
    "count = 0\n",
    "# Process entries\n",
    "for entry in fetch_entries():\n",
    "    url_id, full_url, s3_html_key, s3_screenshot_key, status, source = entry\n",
    "    count += 1\n",
    "        \n",
    "    if count % 1000 == 0:\n",
    "        h5_file.flush()\n",
    "        print(\"\")\n",
    "        print(f\"Count {count}\")\n",
    "        time.sleep(5)\n",
    "    \n",
    "    parsed_url = urlparse(full_url)\n",
    "    \n",
    "    # Construct the URL without query parameters\n",
    "    target_url = f\"{parsed_url.scheme}://{parsed_url.netloc}{parsed_url.path}\"\n",
    "    \n",
    "    # Get the domain\n",
    "    domain = parsed_url.netloc\n",
    "    \n",
    "    # Check if URL is already in HDF5 dataset\n",
    "    if target_url in h5_file['urls']:\n",
    "        continue\n",
    "        \n",
    "    blocked_domains = [\"pages.dev\", \"github.io\", \"weebly.com\", \"vercel.app\", \"weeblysite.com\", \"gitbook.io\" ]\n",
    "    if any(blocked_domain in domain for blocked_domain in blocked_domains):\n",
    "        print(\"x\", end =\"\")\n",
    "        continue\n",
    "    \n",
    "    # Download screenshot from S3\n",
    "    try:\n",
    "        screenshot_obj = s3_client.get_object(Bucket=AWS_S3_BUCKET, Key=s3_screenshot_key)\n",
    "        screenshot_image = Image.open(BytesIO(screenshot_obj['Body'].read()))\n",
    "        screenshot_image = screenshot_image.resize((680, 340))  # Resize image to 680x340\n",
    "        screenshot_array = np.array(screenshot_image)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching screenshot for URL {target_url}: {e}\")\n",
    "        continue\n",
    "        \n",
    "    # Download and parse HTML from S3\n",
    "    try:\n",
    "        html_obj = s3_client.get_object(Bucket=AWS_S3_BUCKET, Key=s3_html_key)\n",
    "        html_content = html_obj['Body'].read().decode('utf-8')\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        main_html_content = soup.prettify()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching HTML for URL {target_url}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    new_index = urls_dataset.shape[0]\n",
    "    urls_dataset.resize((new_index + 1,))\n",
    "    urls_dataset[new_index] = target_url\n",
    "    \n",
    "    screenshots_dataset.resize((new_index + 1, 340, 680, 3))\n",
    "    screenshots_dataset[new_index] = screenshot_array\n",
    "\n",
    "    html_content_dataset.resize((new_index + 1,))\n",
    "    html_content_dataset[new_index] = main_html_content\n",
    "\n",
    "    status_dataset.resize((new_index + 1,))\n",
    "    status_dataset[new_index] = status\n",
    "    \n",
    "    source_dataset.resize((new_index + 1,))\n",
    "    source_dataset[new_index] = source\n",
    "    print(\".\", end =\"\")\n",
    "\n",
    "# Close HDF5 file\n",
    "h5_file.close()\n"
   ],
   "id": "58f6ec9229270d33",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......x...x..xx....x......x...x.x.xx.......x.....x."
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
