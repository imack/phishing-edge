{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "All collection of webpages is done in a completely separate repo I'm calling \"out of scope\" for the project. But the high-level details are that it had a queue of top web pages, would crawl those, and monitor the open phish feed. In both cases, it would render the page in selenium, take a screenshot, and save the HTML to S3. It would save the metadata in a postgres table, which this notebook uses to create the hdf5 dataset we use.",
   "id": "9e070708fb796ddd"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-24T20:36:10.758884Z",
     "start_time": "2024-10-24T20:36:10.432032Z"
    }
   },
   "source": [
    "import psycopg2\n",
    "import boto3\n",
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from bs4 import BeautifulSoup\n",
    "from io import BytesIO\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T20:46:43.222923Z",
     "start_time": "2024-10-24T20:46:43.218693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Database and AWS configuration\n",
    "AWS_S3_BUCKET = 'stingray-phishing-dataset'\n",
    "HDF5_FILE_PATH = 'phishing.h5.nosync'"
   ],
   "id": "e6feaa922ed00a86",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Connect to the database\n",
    "def get_db_connection():\n",
    "    db_string = os.getenv('RDS_CONNECTION')\n",
    "    return psycopg2.connect(db_string)"
   ],
   "id": "9022bec0842dae06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Fetch entries from the database\n",
    "def fetch_entries():\n",
    "    with get_db_connection() as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\"\"\"\n",
    "                SELECT id, full_url, s3_html_key, s3_screenshot_key, source\n",
    "                FROM urls\n",
    "                WHERE s3_html_key IS NOT NULL AND phash_distance > 10 AND source IN (1, 3, 4, 5)\n",
    "            \"\"\")\n",
    "            return cur.fetchall()"
   ],
   "id": "2a810fe02172449b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create or open the HDF5 file\n",
    "h5_file = h5py.File(HDF5_FILE_PATH, 'a')\n",
    "\n",
    "if 'urls' not in h5_file:\n",
    "    h5_file.create_dataset('urls', (0,), maxshape=(None,), dtype=h5py.string_dtype())\n",
    "    h5_file.create_dataset('screenshots', (0, 340, 680, 3), maxshape=(None, 340, 680, 3), dtype='uint8')\n",
    "    h5_file.create_dataset('html_content', (0,), maxshape=(None,), dtype=h5py.string_dtype())\n",
    "    h5_file.create_dataset('source', (0,), maxshape=(None,), dtype='i') \n",
    "    # { internal: 0, open_phish: 1, api: 2, alexa: 3, product_hunt: 4, expansion: 5 }\n"
   ],
   "id": "3773d986896376e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import redis\n",
    "from collections import Counter\n",
    "\n",
    "r = redis.Redis(host='localhost', port=6379, db=0)\n",
    "# Define expiry time (30 days in seconds)\n",
    "expiry_time = 30 * 24 * 60 * 60  # 30 days\n",
    "namespace = \"phish_dataset\"\n"
   ],
   "id": "4cd7cddee9e45276",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time \n",
    "from urllib.parse import urlparse\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "urls_dataset = h5_file['urls']\n",
    "screenshots_dataset = h5_file['screenshots']\n",
    "html_content_dataset = h5_file['html_content']\n",
    "source_dataset = h5_file['source']\n",
    "\n",
    "start_time = time.time()\n",
    "count = 0\n",
    "# Process entries\n",
    "for entry in fetch_entries():\n",
    "    url_id, full_url, s3_html_key, s3_screenshot_key, source = entry\n",
    "    count += 1\n",
    "        \n",
    "    if count % 1000 == 0:\n",
    "        h5_file.flush()\n",
    "        execution_time = time.time() - start_time\n",
    "        print(\"\")\n",
    "        print(f\"Count {count}; {execution_time} seconds\")\n",
    "        time.sleep(5)\n",
    "        start_time = time.time()\n",
    "    \n",
    "    \n",
    "    parsed_url = urlparse(full_url)\n",
    "    \n",
    "    # Construct the URL without query parameters\n",
    "    target_url = f\"{parsed_url.scheme}://{parsed_url.netloc}{parsed_url.path}\"\n",
    "    redis_key = f\"{namespace}:{target_url}\"\n",
    "    \n",
    "    # Get the domain\n",
    "    domain = parsed_url.netloc\n",
    "    \n",
    "    # Check if URL is already in HDF5 dataset\n",
    "    if r.exists(redis_key):\n",
    "        print(\"-\", end =\"\")\n",
    "        continue\n",
    "        \n",
    "    blocked_domains = [\"pages.dev\", \"github.io\", \"weebly.com\", \"vercel.app\", \"weeblysite.com\", \"gitbook.io\" ]\n",
    "    if any(blocked_domain in domain for blocked_domain in blocked_domains):\n",
    "        r.setex(redis_key, expiry_time, target_url)\n",
    "        print(\"x\", end =\"\")\n",
    "        continue\n",
    "    \n",
    "    # Download screenshot from S3\n",
    "    try:\n",
    "        screenshot_obj = s3_client.get_object(Bucket=AWS_S3_BUCKET, Key=s3_screenshot_key)\n",
    "        screenshot_image = Image.open(BytesIO(screenshot_obj['Body'].read()))\n",
    "        screenshot_image = screenshot_image.resize((680, 340))  # Resize image to 680x340\n",
    "        screenshot_array = np.array(screenshot_image)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching screenshot for URL {target_url}: {e}\")\n",
    "        continue\n",
    "        \n",
    "    # Download and parse HTML from S3\n",
    "    try:\n",
    "        html_obj = s3_client.get_object(Bucket=AWS_S3_BUCKET, Key=s3_html_key)\n",
    "        html_content = html_obj['Body'].read().decode('utf-8')\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        main_html_content = soup.prettify()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching HTML for URL {target_url}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    new_index = urls_dataset.shape[0]\n",
    "    urls_dataset.resize((new_index + 1,))\n",
    "    urls_dataset[new_index] = target_url\n",
    "    \n",
    "    source_dataset.resize((new_index + 1,))\n",
    "    source_dataset[new_index] = source\n",
    "    \n",
    "    screenshots_dataset.resize((new_index + 1, 340, 680, 3))\n",
    "    screenshots_dataset[new_index] = screenshot_array[:, :, :3]\n",
    "\n",
    "    html_content_dataset.resize((new_index + 1,))\n",
    "    html_content_dataset[new_index] = main_html_content.replace('\\x00', '')\n",
    "\n",
    "    \n",
    "    r.setex(redis_key, expiry_time, target_url)\n",
    "    print(\".\", end =\"\")\n",
    "\n",
    "# Close HDF5 file\n",
    "h5_file.close()\n"
   ],
   "id": "58f6ec9229270d33",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....xx............................x......x.......................x.."
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T17:00:18.311905Z",
     "start_time": "2024-10-24T17:00:18.282089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "h5_file = h5py.File(HDF5_FILE_PATH, 'a')\n",
    "urls_dataset = h5_file['urls']\n",
    "screenshots_dataset = h5_file['screenshots']\n",
    "html_content_dataset = h5_file['html_content']\n",
    "source_dataset = h5_file['source']\n",
    "\n",
    "# Use Counter to count the frequency of each unique value\n",
    "print(source_dataset.shape)\n",
    "print(urls_dataset.shape)\n",
    "print(screenshots_dataset.shape)\n",
    "print(html_content_dataset.shape)\n",
    "\n",
    "source_dataset = h5_file['source'][:]\n",
    "\n",
    "# Use Counter to count the frequency of each unique value\n",
    "frequencies = Counter(source_dataset)\n",
    "\n",
    "# Print the frequencies\n",
    "for value, count in frequencies.items():\n",
    "    print(f\"Value: {value}, Count: {count}\")"
   ],
   "id": "9e0a334f60bd99e7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72083,)\n",
      "(72083,)\n",
      "(72083, 340, 680, 3)\n",
      "(72083,)\n",
      "Value: 1, Count: 26670\n",
      "Value: 5, Count: 22728\n",
      "Value: 3, Count: 21500\n",
      "Value: 4, Count: 1185\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6bf87c612bb44fb2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now let's make us of the dataset to make a final dataset we can use for testing et. al  ",
   "id": "b0496fcc2c02f9c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Function to resize datasets\n",
    "def resize_dataset(dataset, new_size):\n",
    "    dataset.resize((new_size, *dataset.shape[1:]))\n",
    "\n",
    "# File paths\n",
    "HDF5_FILE_PATH = 'phishing.h5.nosync'\n",
    "NEW_HDF5_FILE_PATH = '/tmp/phishing_output.h5'\n",
    "\n",
    "# Chunk size for reading data in batches\n",
    "CHUNK_SIZE = 1024 * 2\n",
    "\n",
    "# Open original file and create new one\n",
    "with h5py.File(HDF5_FILE_PATH, 'r') as h5_file:\n",
    "    total_entries = h5_file['source'].shape[0]\n",
    "\n",
    "    with h5py.File(NEW_HDF5_FILE_PATH, 'w') as new_h5_file:\n",
    "        # Create new datasets for \"producthunt\", \"train\", \"dev\", \"test\"\n",
    "        for name in ['producthunt', 'train', 'dev', 'test']:\n",
    "            new_h5_file.create_dataset(f'{name}/urls', (0,), maxshape=(None,), dtype=h5py.string_dtype())\n",
    "            new_h5_file.create_dataset(f'{name}/screenshots', (0, 340, 680, 3), maxshape=(None, 340, 680, 3), dtype='uint8')\n",
    "            new_h5_file.create_dataset(f'{name}/html_content', (0,), maxshape=(None,), dtype=h5py.string_dtype())\n",
    "            new_h5_file.create_dataset(f'{name}/labels', (0,), maxshape=(None,), dtype='i')\n",
    "\n",
    "        current_indices = {'producthunt': 0, 'train': 0, 'dev': 0, 'test': 0}\n",
    "\n",
    "        # Iterate over the original dataset in chunks\n",
    "        for i in range(0, total_entries, CHUNK_SIZE):\n",
    "\n",
    "            # Load chunk data\n",
    "            urls_chunk = h5_file['urls'][i:i + CHUNK_SIZE]\n",
    "            screenshots_chunk = h5_file['screenshots'][i:i + CHUNK_SIZE]\n",
    "            html_chunk = h5_file['html_content'][i:i + CHUNK_SIZE]\n",
    "            source_chunk = h5_file['source'][i:i + CHUNK_SIZE]\n",
    "\n",
    "            # Process each entry in the chunk\n",
    "            for j in range(len(source_chunk)):\n",
    "                source = source_chunk[j]\n",
    "                label = 1 if source == 1 else 0\n",
    "\n",
    "                if source == 4:\n",
    "                    dataset_name = 'producthunt'\n",
    "                    print('-', end =\"\")\n",
    "                else:\n",
    "                    rand_val = np.random.rand()\n",
    "                    print('.', end =\"\")\n",
    "                    if rand_val < 0.8:\n",
    "                        dataset_name = 'train'\n",
    "                    elif rand_val < 0.9:\n",
    "                        dataset_name = 'dev'\n",
    "                    else:\n",
    "                        dataset_name = 'test'\n",
    "\n",
    "                idx = current_indices[dataset_name]\n",
    "\n",
    "                # Resize dataset to accommodate new entry\n",
    "                resize_dataset(new_h5_file[f'{dataset_name}/urls'], idx + 1)\n",
    "                resize_dataset(new_h5_file[f'{dataset_name}/screenshots'], idx + 1)\n",
    "                resize_dataset(new_h5_file[f'{dataset_name}/html_content'], idx + 1)\n",
    "                resize_dataset(new_h5_file[f'{dataset_name}/labels'], idx + 1)\n",
    "\n",
    "                # Write data to new dataset\n",
    "                new_h5_file[f'{dataset_name}/urls'][idx] = urls_chunk[j]\n",
    "                new_h5_file[f'{dataset_name}/screenshots'][idx] = screenshots_chunk[j]\n",
    "                new_h5_file[f'{dataset_name}/html_content'][idx] = html_chunk[j]\n",
    "                new_h5_file[f'{dataset_name}/labels'][idx] = label\n",
    "\n",
    "                # Update index for the dataset\n",
    "                current_indices[dataset_name] += 1\n",
    "            \n",
    "            clear_output(wait=True)\n",
    "            print(f\"Chunk: {i}/{total_entries}\")\n",
    "\n",
    "print(\"Dataset splitting completed.\")\n"
   ],
   "id": "f4fc8e177a2b25d2",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splitting completed.\n"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
