{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Preprocess an HDF5 file that was created in datasetcreation.ipynb by cleaning up the HTML, then tokenizing the content into input IDs and attention masks.\n",
    "Save the processed data into a new HDF5 file. This also converts the screenshots into a normalized tensor for the image and create longformer tokens as well. This speeds up iterations because the dataset objects don't need to tokenize text at training time."
   ],
   "id": "2b2b8e6f15f76fad"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-11-28T18:18:15.952073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer, LongformerTokenizer\n",
    "from tqdm import tqdm\n",
    "from custom_html_parser import CustomHTML2Text\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "original_file_path = '/Users/imack/transfer/phishing_output.h5'\n",
    "new_file_path = '/Users/imack/transfer/phishing_output_tokenized.h5'\n",
    "\n",
    "# Classes that do our transformations\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "longformer_tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "\n",
    "converter = CustomHTML2Text()\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def tokenize_with_overlap(html_content, max_chunk_length=512, stride=256):\n",
    "    if not html_content:\n",
    "        html_content = '<html></html>'\n",
    "    \n",
    "    tokens = tokenizer(html_content, add_special_tokens=False, return_tensors='np')[\"input_ids\"].squeeze()\n",
    "\n",
    "    chunks = []\n",
    "    attention_masks = []\n",
    "    for i in range(0, len(tokens), stride):\n",
    "        chunk = tokens[i:i + max_chunk_length]\n",
    "        padded_chunk = np.pad(\n",
    "            chunk,\n",
    "            (0, max(0, max_chunk_length - len(chunk))), \n",
    "            constant_values=tokenizer.pad_token_id\n",
    "        )\n",
    "        chunks.append(padded_chunk)\n",
    "        attention_mask = [1] * len(chunk) + [0] * (max_chunk_length - len(chunk))\n",
    "        attention_masks.append(attention_mask)\n",
    "    \n",
    "    # Convert to arrays of consistent shape\n",
    "    chunks = np.array(chunks, dtype=np.int32)\n",
    "    attention_masks = np.array(attention_masks, dtype=np.int32)\n",
    "    \n",
    "    return chunks, attention_masks\n",
    "\n",
    "with h5py.File(original_file_path, 'r') as original_file, h5py.File(new_file_path, 'a') as outfile:\n",
    "    # Iterate through slices (train, dev, test)\n",
    "    for slice_name in original_file.keys():\n",
    "        slice_group = original_file[slice_name]\n",
    "        print(f\"Processing slice: {slice_name}\")\n",
    "        \n",
    "        if slice_name not in outfile:\n",
    "            new_group = outfile.create_group(slice_name)\n",
    "            new_group.create_dataset('labels', data=slice_group['labels'][:])\n",
    "            new_group.create_dataset('urls', data=slice_group['urls'][:])\n",
    "            new_group.create_dataset('last_processed_index', data=np.array([-1], dtype=np.int32))\n",
    "        else:\n",
    "            new_group = outfile[slice_name]\n",
    "        \n",
    "        # Load last processed index, in case we need to restart\n",
    "        last_processed_index = new_group['last_processed_index'][0]\n",
    "        \n",
    "        html_contents = slice_group['html_content'][:]\n",
    "        urls = slice_group['urls'][:]\n",
    "        screenshots = slice_group['screenshots'][:] # raw screenshots\n",
    "        \n",
    "        # Create new datasets for processed data if they don't exist\n",
    "        if 'html_input_ids' not in new_group:\n",
    "            input_ids_dataset = new_group.create_dataset(\n",
    "                \"html_input_ids\", \n",
    "                shape=(0,), \n",
    "                maxshape=(None,), \n",
    "                dtype=h5py.special_dtype(vlen=np.dtype('int32'))\n",
    "            )\n",
    "            attention_masks_dataset = new_group.create_dataset(\n",
    "                \"html_attention_masks\", \n",
    "                shape=(0,), \n",
    "                maxshape=(None,), \n",
    "                dtype=h5py.special_dtype(vlen=np.dtype('int32'))\n",
    "            )\n",
    "            url_input_ids_dataset = new_group.create_dataset(\n",
    "                \"url_input_ids\", \n",
    "                shape=(0, 128),  \n",
    "                maxshape=(None, 128), \n",
    "                dtype=np.int32\n",
    "            )\n",
    "            \n",
    "            longformer_attention_masks_dataset = new_group.create_dataset(\n",
    "                \"longformer_attention_masks\", \n",
    "                shape=(0, 4096),  \n",
    "                maxshape=(None, 4096), \n",
    "                dtype=np.int32\n",
    "            )\n",
    "            longformer_input_ids_dataset = new_group.create_dataset(\n",
    "                \"longformer_input_ids\", \n",
    "                shape=(0, 4096),  \n",
    "                maxshape=(None, 4096), \n",
    "                dtype=np.int32\n",
    "            )\n",
    "            url_attention_masks_dataset = new_group.create_dataset(\n",
    "                \"url_attention_masks\", \n",
    "                shape=(0, 128), \n",
    "                maxshape=(None, 128), \n",
    "                dtype=np.int32\n",
    "            )\n",
    "            html_content_dataset = new_group.create_dataset(\n",
    "                \"html_content\",\n",
    "                shape=(0,),\n",
    "                maxshape=(None,),\n",
    "                dtype=h5py.string_dtype(encoding=\"utf-8\"),\n",
    "                chunks=(1,)\n",
    "            )\n",
    "\n",
    "            image_dataset = new_group.create_dataset(\n",
    "                \"images\",\n",
    "                shape=(0,3,340,680),\n",
    "                maxshape=(None,3,340,680),\n",
    "                dtype=np.float32\n",
    "            )\n",
    "        else:\n",
    "            input_ids_dataset = new_group['html_input_ids']\n",
    "            attention_masks_dataset = new_group['html_attention_masks']\n",
    "            longformer_input_ids_dataset = new_group['longformer_input_ids']\n",
    "            longformer_attention_masks_dataset = new_group['longformer_attention_masks']\n",
    "            url_input_ids_dataset = new_group['url_input_ids']\n",
    "            url_attention_masks_dataset = new_group['url_attention_masks']\n",
    "            html_content_dataset = new_group['html_content']\n",
    "            image_dataset = new_group['images']\n",
    "        \n",
    "        print(f\"last_processed_index: {last_processed_index}\")\n",
    "        # Resume processing from the last processed index\n",
    "        for i in tqdm(range(last_processed_index + 1, len(html_contents)), total=len(html_contents)):\n",
    "            html_content = html_contents[i].decode('utf-8')        \n",
    "            \n",
    "            plain_text = converter.handle(html_content)\n",
    "            \n",
    "            chunks, attention_masks = tokenize_with_overlap(plain_text)\n",
    "            url = urls[i].decode('utf-8')\n",
    "            \n",
    "            flat_input_ids = np.concatenate(chunks).astype(np.int32)\n",
    "            flat_attention_masks = np.concatenate(attention_masks).astype(np.int32)\n",
    "            \n",
    "            input_ids_dataset.resize((input_ids_dataset.shape[0] + 1,))\n",
    "            attention_masks_dataset.resize((attention_masks_dataset.shape[0] + 1,))\n",
    "            input_ids_dataset[-1] = flat_input_ids\n",
    "            attention_masks_dataset[-1] = flat_attention_masks\n",
    "            \n",
    "            longformer_inputs = longformer_tokenizer(plain_text, return_tensors=\"np\", max_length=4096, truncation=True, padding=\"max_length\")\n",
    "            \n",
    "            longformer_input_ids_dataset.resize((longformer_input_ids_dataset.shape[0] + 1,4096))\n",
    "            longformer_attention_masks_dataset.resize((longformer_attention_masks_dataset.shape[0] + 1,4096))\n",
    "            \n",
    "            longformer_input_ids_dataset[-1] = longformer_inputs['input_ids']\n",
    "            longformer_attention_masks_dataset[-1] = longformer_inputs['attention_mask']\n",
    "\n",
    "            image_dataset.resize((image_dataset.shape[0] + 1,3,340,680))\n",
    "            image_dataset[-1] = transform(screenshots[i])\n",
    "\n",
    "            encoded_url_input = tokenizer(\n",
    "                url,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=128,\n",
    "                return_tensors='np'\n",
    "            )\n",
    "            \n",
    "            html_content_dataset.resize((html_content_dataset.shape[0] + 1, ))\n",
    "            html_content_dataset[-1] = plain_text\n",
    "            \n",
    "            url_input_ids_dataset.resize((url_input_ids_dataset.shape[0] + 1, 128))\n",
    "            url_input_ids_dataset[-1, :] = encoded_url_input['input_ids']\n",
    "            \n",
    "            url_attention_masks_dataset.resize((url_attention_masks_dataset.shape[0] + 1, 128))\n",
    "            url_attention_masks_dataset[-1, :] = encoded_url_input['attention_mask']\n",
    "            \n",
    "            new_group['last_processed_index'][0] = i\n",
    "            outfile.flush() \n",
    "\n",
    "print(\"Preprocessing complete. Saved to\", new_file_path)\n"
   ],
   "id": "6894ff755f571417",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/phishing-edge/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing slice: dev\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d5fa7c8cd61eab1b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
